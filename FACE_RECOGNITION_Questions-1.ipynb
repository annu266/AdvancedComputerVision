{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FACE RECOGNITION_Questions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "FZPrUBuA2D9n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deep face recognition with Keras"
      ]
    },
    {
      "metadata": {
        "id": "7jpaCy9Q1y4M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### First, lets install the required libraries. Upload the `requirements.txt` file given and run the below commands."
      ]
    },
    {
      "metadata": {
        "id": "e9k1hVvWLnGw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt --user"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N0YcTuo3MmBS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install request"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1aI_DPyNaeaX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Installing Dlib"
      ]
    },
    {
      "metadata": {
        "id": "0uZqN-rFXknw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt install python python-pip build-essential cmake pkg-config libx11-dev libatlas-base-dev libgtk-3-dev libboost-python-dev -y\n",
        "\n",
        "!pip install dlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BXiNriJXOYCa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download Dlib's face landmarks data file for running face alignment.\n",
        "\n",
        "This will helps us in aligning faces before we learn the features for each face. **`Run the below code.`** It will create a directory with name **`models` **and save **`landmarks.dat`** file in that folder."
      ]
    },
    {
      "metadata": {
        "id": "iOAofB2nLoii",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import bz2\n",
        "import os\n",
        "\n",
        "from urllib.request import urlopen\n",
        "\n",
        "def download_landmarks(dst_file):\n",
        "    url = 'http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2'\n",
        "    decompressor = bz2.BZ2Decompressor()\n",
        "    \n",
        "    with urlopen(url) as src, open(dst_file, 'wb') as dst:\n",
        "        data = src.read(1024)\n",
        "        while len(data) > 0:\n",
        "            dst.write(decompressor.decompress(data))\n",
        "            data = src.read(1024)\n",
        "\n",
        "dst_dir = 'models'\n",
        "dst_file = os.path.join(dst_dir, 'landmarks.dat')\n",
        "\n",
        "if not os.path.exists(dst_file):\n",
        "    os.makedirs(dst_dir)\n",
        "    download_landmarks(dst_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bNbjzAgh3em-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the network\n",
        "\n",
        "The CNN model is taken from the Keras-OpenFace project. The architecture details aren't too important here, it's only useful to know that there is a fully connected layer with 128 hidden units followed by an L2 normalization layer on top of the convolutional base. These two top layers are referred to as the embedding layer from which the 128-dimensional embedding vectors can be obtained. The complete model is defined in `model.py` and a graphical overview is given in `model.png`. A Keras version of the `nn4.small2` model can be created with `create_model()`.\n",
        "\n",
        "\n",
        "**Run the below code to initialize the model**"
      ]
    },
    {
      "metadata": {
        "id": "cRMdAbSsMcPC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from model import create_model\n",
        "\n",
        "nn4_small2 = create_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XyCaIac9N8_8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Idea of Training the model with Triplet loss function "
      ]
    },
    {
      "metadata": {
        "id": "fNKudbMe4u7W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model training aims to learn an embedding f(x) of image x such that the squared L2 distance between all faces of the same identity is small and the distance between a pair of faces from different identities is large. This can be achieved with a triplet loss L that is minimized when the distance between an anchor image xai and a positive image xpi (same identity) in embedding space is smaller than the distance between that anchor image and a negative image xni (different identity) by at least a margin Î±."
      ]
    },
    {
      "metadata": {
        "id": "2Ose4tTyPDeU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Layer\n",
        "\n",
        "# Input for anchor, positive and negative images\n",
        "in_a = Input(shape=(96, 96, 3))\n",
        "in_p = Input(shape=(96, 96, 3))\n",
        "in_n = Input(shape=(96, 96, 3))\n",
        "\n",
        "# Output for anchor, positive and negative embedding vectors\n",
        "# The nn4_small model instance is shared (Siamese network)\n",
        "emb_a = nn4_small2(in_a)\n",
        "emb_p = nn4_small2(in_p)\n",
        "emb_n = nn4_small2(in_n)\n",
        "\n",
        "class TripletLossLayer(Layer):\n",
        "    def __init__(self, alpha, **kwargs):\n",
        "        self.alpha = alpha\n",
        "        super(TripletLossLayer, self).__init__(**kwargs)\n",
        "    \n",
        "    def triplet_loss(self, inputs):\n",
        "        a, p, n = inputs\n",
        "        p_dist = K.sum(K.square(a-p), axis=-1)\n",
        "        n_dist = K.sum(K.square(a-n), axis=-1)\n",
        "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        loss = self.triplet_loss(inputs)\n",
        "        self.add_loss(loss)\n",
        "        return loss\n",
        "\n",
        "# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\n",
        "triplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n",
        "\n",
        "# Model that can be trained with anchor, positive negative images\n",
        "nn4_small2_train = Model([in_a, in_p, in_n], triplet_loss_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qpTO6eimP5t0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from data import triplet_generator\n",
        "\n",
        "# triplet_generator() creates a generator that continuously returns \n",
        "# ([a_batch, p_batch, n_batch], None) tuples where a_batch, p_batch \n",
        "# and n_batch are batches of anchor, positive and negative RGB images \n",
        "# each having a shape of (batch_size, 96, 96, 3).\n",
        "generator = triplet_generator() \n",
        "\n",
        "nn4_small2_train.compile(loss=None, optimizer='adam')\n",
        "nn4_small2_train.fit_generator(generator, epochs=10, steps_per_epoch=100)\n",
        "\n",
        "# Please note that the current implementation of the generator only generates \n",
        "# random image data. The main goal of this code snippet is to demonstrate \n",
        "# the general setup for model training. In the following, we will anyway \n",
        "# use a pre-trained model so we don't need a generator here that operates \n",
        "# on real training data."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AdsuNBe9OHyZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For this project, we are considering a pre-trained model given in file path **`nn4.small2.v1.h5`**.\n",
        "\n",
        "Write code: Using **load_weights()** function load the given pre-trained weight file."
      ]
    },
    {
      "metadata": {
        "id": "oV73nKP3QCLT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bulEElwQ54g6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ]
    },
    {
      "metadata": {
        "id": "re9AUK4f520z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To demonstrate face recognition on a custom dataset, a small dataset is used. It consists of around 15-25 face images of 10 different persons. The metadata for each image (file and identity name) are loaded into memory for later processing.\n",
        "\n",
        "\n",
        "Upload Images zip file given to drive and download and extract it using the below code. And we will pass the folder `images` to `load_metadata` function to save all the images filenames and person numbers."
      ]
    },
    {
      "metadata": {
        "id": "sbDurWRMQnw7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Import drive module from google.colab"
      ]
    },
    {
      "metadata": {
        "id": "KYr8qfMD7Poc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0GlBCA8wQ1cX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Give a path to mount the files in your drive"
      ]
    },
    {
      "metadata": {
        "id": "ZxhxrGQS7Pq2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CBB_OncAQ8h_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Using the above given mounted path, give the images.zip path dependent on where you placed the file in your drive."
      ]
    },
    {
      "metadata": {
        "id": "pprDxdHT7S6N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## For example\n",
        "images_path = \"/content/drive/My Drive/DLCP/Project-2/images.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "edsF05iuRkOd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Using ZipFile module to extract the images zip file"
      ]
    },
    {
      "metadata": {
        "id": "m3eIglFdVcvB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CVAjzG4IXYQX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with ZipFile(images_path, 'r') as zip:\n",
        "  zip.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oesXJD9ySB6w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Run the below function to load the images from the extracted images folder from the above step and map each image with person id \n"
      ]
    },
    {
      "metadata": {
        "id": "4Q7TS19vVbGb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os.path\n",
        "\n",
        "class IdentityMetadata():\n",
        "    def __init__(self, base, name, file):\n",
        "        # print(base, name, file)\n",
        "        # dataset base directory\n",
        "        self.base = base\n",
        "        # identity name\n",
        "        self.name = name\n",
        "        # image file name\n",
        "        self.file = file\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.image_path()\n",
        "\n",
        "    def image_path(self):\n",
        "        return os.path.join(self.base, self.name, self.file) \n",
        "    \n",
        "def load_metadata(path):\n",
        "    metadata = []\n",
        "    for i in os.listdir(path):\n",
        "        for f in os.listdir(os.path.join(path, i)):\n",
        "            # Check file extension. Allow only jpg/jpeg' files.\n",
        "            ext = os.path.splitext(f)[1]\n",
        "            if ext == '.jpg' or ext == '.jpeg':\n",
        "                metadata.append(IdentityMetadata(path, i, f))\n",
        "    return np.array(metadata)\n",
        "\n",
        "metadata = load_metadata('images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vr4xVNqIaHgE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Face alignment\n",
        "The nn4.small2.v1 model was trained with aligned face images, therefore, the face images from the custom dataset must be aligned too. Here, we use Dlib for face detection and OpenCV for image transformation and cropping to produce aligned 96x96 RGB face images. We are using the AlignDlib utility from the OpenFace project."
      ]
    },
    {
      "metadata": {
        "id": "vZJXEFz9UaAR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### 1. Run the below code to import AlignDlib\n",
        "\n",
        "For this you need align.py available in the environment."
      ]
    },
    {
      "metadata": {
        "id": "DFCOKB1AUq0c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from align import AlignDlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tsTnKz_zWVM_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use the landmarks data file downloaded in the first steps for face alignment. file path **`models/landmarks.dat`**"
      ]
    },
    {
      "metadata": {
        "id": "ijh2N8QxWSN4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize the OpenFace face alignment utility\n",
        "alignment = AlignDlib('models/landmarks.dat')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JbWhL7jbUwHg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### 2. Run the beloiw code to  load an image from the metadata created in the step before Face Alignment\n",
        "\n",
        "You can access each image path from `metadata[i].image_path()` where, i is the image number. i can take values from 1 to no.of images in the dataset given."
      ]
    },
    {
      "metadata": {
        "id": "ape5WxvVWKOe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "def load_image(path):\n",
        "    img = cv2.imread(path, 1)\n",
        "    # OpenCV loads images with color channels\n",
        "    # in BGR order. So we need to reverse them\n",
        "    return img[...,::-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ptDNq8noWK89",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load an image\n",
        "# for example, loading the image with index 1\n",
        "one_image = load_image(metadata[0].image_path())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fpe2G6JSbTis",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Write code to load 2nd and 3rd images in the metadata using load_image()"
      ]
    },
    {
      "metadata": {
        "id": "vEEzp1SkbdxI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f0ywYChhXHQI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### 3. Run the below code to align the above loaded image."
      ]
    },
    {
      "metadata": {
        "id": "0_GO6xugXHd8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Detect face and return bounding box\n",
        "bb = alignment.getLargestFaceBoundingBox(one_image)\n",
        "\n",
        "# Transform image using specified face landmark indices and crop image to 96x96\n",
        "one_image_aligned = alignment.align(96, one_image, bb, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Show original image\n",
        "plt.subplot(131)\n",
        "plt.imshow(one_image)\n",
        "\n",
        "# Show original image with bounding box\n",
        "plt.subplot(132)\n",
        "plt.imshow(one_image)\n",
        "plt.gca().add_patch(patches.Rectangle((bb.left(), bb.top()), bb.width(), bb.height(), fill=False, color='red'))\n",
        "\n",
        "# Show aligned image\n",
        "plt.subplot(133)\n",
        "plt.imshow(one_image_aligned);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w5BrN6vpX_kj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Write a function image_align() which take image path as input and returns the aligned image in output.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "inrLd5JCYYB2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LkBQRL_sd2U8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate embeddings for each image in the dataset\n",
        "\n",
        "Given below is an example to load the first image in the metadata and get its embedding vector from the pre-trained model. "
      ]
    },
    {
      "metadata": {
        "id": "S01r8UzXc-8s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Get embedding vector for first image in the metadata using the pre-trained model"
      ]
    },
    {
      "metadata": {
        "id": "B2yd69OydBAq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Align the image\n",
        "img_aligned = image_align(metadata[0].image_path())\n",
        "\n",
        "# Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\n",
        "img = (img_aligned / 255.).astype(np.float32)\n",
        "\n",
        "# obtain embedding vector for an image\n",
        "embedding_vector = nn4_small2_pretrained.predict(np.expand_dims(img, axis=0))[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "plHvUTytcTGo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Write code to iterate through metadata and create embeddings for each image using nn4_small2_pretrained.predict() and store in a list with name `embeddings`\n",
        "\n",
        "If there is any error in reading any image in the dataset, fill the emebdding vector of that image with 128-zeroes as the final embedding from the model is of length 128."
      ]
    },
    {
      "metadata": {
        "id": "yY9ykxtueY4k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4hb3XSDsfTMG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Write code to get the distance between given 2 pairs of images.\n",
        "\n",
        "Consider distance metric as \"Squared L2 distance\"\n",
        "\n",
        "squared l2 distance between 2 points (x1, y1) and (x2, y2) = (x1-x2)^2 + (y1-y2)^2\n",
        "\n",
        "\n",
        "\n",
        "##### Plot images and get distance between the pairs given below.\n",
        "\n",
        "1. 2,3 and 2,120\n",
        "\n",
        "2. 30,31 and 30,100\n",
        "\n",
        "3. 70,72 and 70,115"
      ]
    },
    {
      "metadata": {
        "id": "KF1OFzRDsP--",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dh1MxMTzsQNB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cMv9vqACsQ12",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N_HfgsQXgV6u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Now lets build a SVM classifier to predict person in the given image. "
      ]
    },
    {
      "metadata": {
        "id": "AahHoLK1hZJj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use LinearSVC in sklearn.svm\n",
        "\n",
        "Run the below code to divide half of the images as training set and another half of the images as test set."
      ]
    },
    {
      "metadata": {
        "id": "2gfLzoBIhrpV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "targets = np.array([m.name for m in metadata])\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(targets)\n",
        "\n",
        "# Numerical encoding of identities\n",
        "y = encoder.transform(targets)\n",
        "\n",
        "train_idx = np.arange(metadata.shape[0]) % 2 != 0\n",
        "test_idx = np.arange(metadata.shape[0]) % 2 == 0\n",
        "\n",
        "## checking the shapes of metaadata and test and train sets\n",
        "print(metadata.shape)\n",
        "print(train_idx.shape)\n",
        "print(test_idx.shape)\n",
        "\n",
        "\n",
        "# one half as train examples of 10 identities\n",
        "X_train = embedding[train_idx]\n",
        "# another half as test examples of 10 identities\n",
        "X_test = embedding[test_idx]\n",
        "\n",
        "y_train = y[train_idx]\n",
        "y_test = y[test_idx]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "312NcUeuiNpL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Build SVM and report the accuracy"
      ]
    },
    {
      "metadata": {
        "id": "6ybglrDph7tJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JqFfYIZbiXG8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Test the classifier\n",
        "\n",
        "Take 35th image from test set and plot the image, report to which person(folder name in dataset) the image belongs to."
      ]
    },
    {
      "metadata": {
        "id": "p2E6WBuMiBmX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}